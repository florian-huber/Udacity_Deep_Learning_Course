{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udacity course - Deep Learning\n",
    "### Assignment 3 \n",
    "Problems and sample code are taken from Udacity's free course on Deep Learning.\n",
    "\n",
    "First the necessary packages are imported. Then the notMNIST dataset is loaded (containing characters A to J)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "data_root = \"C:\\\\Programming\\\\Udacity_Deep_Learning\\\\Exercise_01\" # Change me to store data elsewhere\n",
    "\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to data format: \n",
    "    1) Images from 2D to 1D\n",
    "    2) One hot encoding of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "\n",
    "So, first the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logistic model (only minor modifications to original code to implement regularization)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "reg_scale = 1e-3 #strength of regularization --> important hyperparameter --> tuning\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))                                   \n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Weights.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    base_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    reg_loss = tf.reduce_sum(tf.nn.l2_loss(weights)) #nothing more then ~ sum(weights ** 2) / 2\n",
    "    loss = tf.add(base_loss, reg_scale * reg_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.9).minimize(loss) #Momentum Optimizer instead of GradientDescent\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.773857\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 1000: 0.965446\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2000: 0.621625\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 3000: 0.756445\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 4000: 0.831370\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 5000: 0.702688\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001 \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size) \n",
    "        \n",
    "        # Generate a minibatch:\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next step:\n",
    "Now add l2-regularization to a 1-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple neural networks:\n",
    "\n",
    "reg_scale = 1e-3 #strength of regularization --> important hyperparameter --> tuning\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden1 = 1024 #no of elements in hidden layer 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Weights\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden1]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden1, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    # Hidden layer 1\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1) \n",
    "    \n",
    "    # Output\n",
    "    logits = tf.matmul(hidden1, weights2) + biases2 \n",
    "    \n",
    "    # Loss\n",
    "    base_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    reg_loss = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) #l2 regularization\n",
    "    loss = tf.add(base_loss, reg_scale * reg_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1) #compute hidden layer\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden1_valid, weights2) + biases2)\n",
    "    hidden1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1) #compute hidden layer\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 810.279602\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 30.8%\n",
      "Mean weights1:  0.722253  | Mean weights2:  0.83402\n",
      "Minibatch loss at step 1000: 113.542099\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.0%\n",
      "Mean weights1:  0.431994  | Mean weights2:  0.243169\n",
      "Minibatch loss at step 2000: 41.104588\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.7%\n",
      "Mean weights1:  0.260832  | Mean weights2:  0.132334\n",
      "Minibatch loss at step 3000: 15.566499\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.6%\n",
      "Mean weights1:  0.158188  | Mean weights2:  0.0903077\n",
      "Minibatch loss at step 4000: 6.059772\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Mean weights1:  0.0964016  | Mean weights2:  0.0755019\n",
      "Minibatch loss at step 5000: 2.448214\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.0%\n",
      "Mean weights1:  0.0592797  | Mean weights2:  0.0661187\n",
      "Minibatch loss at step 6000: 1.164128\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.0%\n",
      "Mean weights1:  0.0371566  | Mean weights2:  0.0590759\n",
      "Minibatch loss at step 7000: 0.737109\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.8%\n",
      "Mean weights1:  0.024154  | Mean weights2:  0.0534219\n",
      "Minibatch loss at step 8000: 0.647631\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.9%\n",
      "Mean weights1:  0.0167114  | Mean weights2:  0.0492184\n",
      "Minibatch loss at step 9000: 0.515222\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.2%\n",
      "Mean weights1:  0.0125499  | Mean weights2:  0.0463361\n",
      "Minibatch loss at step 10000: 0.581433\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.4%\n",
      "Mean weights1:  0.0102545  | Mean weights2:  0.0444485\n",
      "Test accuracy: 94.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001 \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch:\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Mean weights1: \", np.mean(tf.abs(weights1).eval()), \n",
    "                  \" | Mean weights2: \", np.mean(tf.abs(weights2).eval()))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we immediately get > 94% with a -still super-simple- neural network.... much better than logistic regression.\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 701.815125\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 36.4%\n",
      "Mean weights1:  0.721942  | Mean weights2:  0.766291\n",
      "Minibatch loss at step 1000: 115.419258\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.5%\n",
      "Mean weights1:  0.437205  | Mean weights2:  0.453178\n",
      "Minibatch loss at step 2000: 42.449886\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.5%\n",
      "Mean weights1:  0.265145  | Mean weights2:  0.274849\n",
      "Minibatch loss at step 3000: 15.614120\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Mean weights1:  0.160802  | Mean weights2:  0.166925\n",
      "Test accuracy: 83.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001 \n",
    "\n",
    "num_batches = 4 # number of mini-batches to train on\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch:\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Mean weights1: \", np.mean(tf.abs(weights1).eval()), \n",
    "                  \" | Mean weights2: \", np.mean(tf.abs(weights2).eval()))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the model performs much worth. \n",
    "It is now overfitting on the little data of the few mini-batches provided for training.\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple neural networks + regularization + dropout\n",
    "\n",
    "reg_scale = 1e-3 #strength of regularization --> important hyperparameter --> tuning\n",
    "dropout_rate = 0.5 \n",
    "\n",
    "batch_size = 128\n",
    "num_hidden1 = 1024 #no of elements in hidden layer 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #only use DROPOUT during training (default is training=False)\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training') \n",
    "    tf_train_dataset_drop = tf.layers.dropout(tf_train_dataset, dropout_rate, training=training)\n",
    "\n",
    "    # Weights.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden1]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden1, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    # Hidden layer 1\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset_drop, weights1) + biases1) \n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training) #only include in training!\n",
    "    \n",
    "    # Output\n",
    "    logits = tf.matmul(hidden1_drop, weights2) + biases2 \n",
    "    \n",
    "    # Loss    \n",
    "    base_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    reg_loss = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) \n",
    "    loss = tf.add(base_loss, reg_scale * reg_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1) #compute hidden layer\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden1_valid, weights2) + biases2)\n",
    "    hidden1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1) #compute hidden layer\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 1011.486694\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 19.6%\n",
      "Mean weights1:  0.722651  | Mean weights2:  0.765303\n",
      "Minibatch loss at step 1000: 133.861786\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 77.2%\n",
      "Mean weights1:  0.445452  | Mean weights2:  0.667341\n",
      "Minibatch loss at step 2000: 46.071102\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 78.6%\n",
      "Mean weights1:  0.271446  | Mean weights2:  0.423325\n",
      "Minibatch loss at step 3000: 17.002235\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.9%\n",
      "Mean weights1:  0.164989  | Mean weights2:  0.261948\n",
      "Test accuracy: 85.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001 \n",
    "\n",
    "num_batches = 4 # number of mini-batches to train on\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch:\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {training : True, tf_train_dataset : batch_data, tf_train_labels : batch_labels} #set training -> True\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Mean weights1: \", np.mean(tf.abs(weights1).eval()), \n",
    "                  \" | Mean weights2: \", np.mean(tf.abs(weights2).eval()))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, when restricted to few mini-batches, we see drastic overfitting!\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "First, let's start with a 3-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deeper NN + regularization + learning rate decay\n",
    "\n",
    "# hyperparameters\n",
    "scale_reg = 1e-4 #strength of regularization\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 5000\n",
    "decay_rate = 1/4\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# network layers\n",
    "num_hidden1 = 800 #no of elements in hidden layer 1\n",
    "num_hidden2 = 400\n",
    "num_hidden3 = 200\n",
    "\n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape, name = None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name = name)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))                               \n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\") \n",
    "\n",
    "    #Layer 1\n",
    "    weights1 = weight_variable([image_size * image_size, num_hidden1])\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden1]))\n",
    "    \n",
    "    #Layer 2\n",
    "    weights2 = weight_variable([num_hidden1, num_hidden2])\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden2]))\n",
    "    \n",
    "    #Layer 3\n",
    "    weights3 = weight_variable([num_hidden2, num_hidden3])\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden3]))\n",
    "    \n",
    "    #Output layer\n",
    "    weights4 = weight_variable([num_hidden3, num_labels])\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "    \n",
    "    # Output\n",
    "    logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "    \n",
    "    # Loss\n",
    "    base_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    reg_loss = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4)\n",
    "    loss = tf.add(base_loss, scale_reg * reg_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 6.234508\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 17.3%\n",
      "Mean weights1:  0.0722632  | Mean weights2:  0.072213\n",
      "Minibatch loss at step 1000: 0.788205\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.5%\n",
      "Mean weights1:  0.0670178  | Mean weights2:  0.0686215\n",
      "Minibatch loss at step 2000: 0.617137\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.5%\n",
      "Mean weights1:  0.0620059  | Mean weights2:  0.0636479\n",
      "Minibatch loss at step 3000: 0.692472\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.6%\n",
      "Mean weights1:  0.0576146  | Mean weights2:  0.0593083\n",
      "Minibatch loss at step 4000: 0.639147\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.3%\n",
      "Mean weights1:  0.0539482  | Mean weights2:  0.0556504\n",
      "Minibatch loss at step 5000: 0.438611\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.9%\n",
      "Mean weights1:  0.0508358  | Mean weights2:  0.052532\n",
      "Minibatch loss at step 6000: 0.405787\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Mean weights1:  0.0497408  | Mean weights2:  0.0514233\n",
      "Minibatch loss at step 7000: 0.389847\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Mean weights1:  0.0487075  | Mean weights2:  0.0503899\n",
      "Minibatch loss at step 8000: 0.413032\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Mean weights1:  0.0477174  | Mean weights2:  0.0494163\n",
      "Minibatch loss at step 9000: 0.368552\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Mean weights1:  0.0467867  | Mean weights2:  0.0485027\n",
      "Minibatch loss at step 10000: 0.349989\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Mean weights1:  0.0459031  | Mean weights2:  0.0476456\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001 \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch:\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Mean weights1: \", np.mean(tf.abs(weights1).eval()), \n",
    "                  \" | Mean weights2: \", np.mean(tf.abs(weights2).eval()))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's becoming quite good ...\n",
    "\n",
    "\n",
    "With increasing numbers of layers as well as additional optimization (dropout, batch normalization etc.), the way of constructing the network layers should better change. It simply becomes too messy if we go on as in the examples so far.\n",
    "\n",
    "So now, instead of using the `tf.nn.` elements I'll switch to `tf.layers.`since those already include a lot of functions. And this way the weights will also be initialized automatically.\n",
    "\n",
    "Next, dropout is added (`nn.layers.dropout`) and the number of neurons is slightly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deeper NN + dropout + regularization + learning rate decay\n",
    "\n",
    "# hyperparameters\n",
    "scale_reg = 0#1e-4 #strength of regularization\n",
    "dropout_rate = 0.3 #0.5\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 5000\n",
    "decay_rate = 1/4\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# network layers\n",
    "num_hidden1 = 1200 #no of elements in hidden layer 1\n",
    "num_hidden2 = 600\n",
    "num_hidden3 = 300\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.int64, shape=(None))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training') #only use DROPOUT during training!\n",
    "\n",
    "    # First dropout layer:\n",
    "    tf_train_dataset_drop = tf.layers.dropout(tf_train_dataset, dropout_rate, training=training)\n",
    "  \n",
    "    # Network layers.\n",
    "    hidden1 = tf.layers.dense(tf_train_dataset_drop, num_hidden1, activation=tf.nn.relu, name=\"hidden1\") #change: not tf.layers... will automatically initiate weights\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)                          \n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, num_hidden2, activation=tf.nn.relu, name=\"hidden2\") #change: not tf.layers... will automatically initiate weights\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(hidden2_drop, num_hidden3, activation=tf.nn.relu, name=\"hidden3\") #change: not tf.layers... will automatically initiate weights\n",
    "    hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)\n",
    "        \n",
    "    # Output\n",
    "    logits = tf.layers.dense(hidden3_drop, num_labels, name=\"outputs\") \n",
    "    \n",
    "    # Loss\n",
    "    #extract weights:\n",
    "    weights1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "    weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "    weights3 = tf.get_default_graph().get_tensor_by_name(\"hidden3/kernel:0\")\n",
    "    weights4 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "    \n",
    "    base_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    reg_loss = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4)\n",
    "    loss = tf.add(base_loss, reg_scale * reg_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)    \n",
    "    \n",
    "    # Evaluate\n",
    "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(tf_train_labels, 1))\n",
    "    accuracy_measure = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also switch from \"num_steps\" to \"epochs\" since this is more easily comparable if we start playing with different batch-sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss after epoch 1: 0.702568\n",
      "Minibatch accuracy:  0.84375  | Validation accuracy:  0.8598\n",
      "Minibatch loss after epoch 2: 0.574617\n",
      "Minibatch accuracy:  0.859375  | Validation accuracy:  0.872\n",
      "Minibatch loss after epoch 3: 0.510364\n",
      "Minibatch accuracy:  0.875  | Validation accuracy:  0.8811\n",
      "Minibatch loss after epoch 4: 0.494948\n",
      "Minibatch accuracy:  0.875  | Validation accuracy:  0.8907\n",
      "Minibatch loss after epoch 5: 0.484861\n",
      "Minibatch accuracy:  0.882813  | Validation accuracy:  0.8937\n",
      "Minibatch loss after epoch 6: 0.458294\n",
      "Minibatch accuracy:  0.890625  | Validation accuracy:  0.898\n",
      "Minibatch loss after epoch 7: 0.444091\n",
      "Minibatch accuracy:  0.898438  | Validation accuracy:  0.9009\n",
      "Minibatch loss after epoch 8: 0.443158\n",
      "Minibatch accuracy:  0.898438  | Validation accuracy:  0.903\n",
      "Minibatch loss after epoch 9: 0.440389\n",
      "Minibatch accuracy:  0.914063  | Validation accuracy:  0.9022\n",
      "Minibatch loss after epoch 10: 0.438266\n",
      "Minibatch accuracy:  0.90625  | Validation accuracy:  0.9038\n",
      "Test accuracy:  0.9573\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 #one epoch = 1x forward pass + backward pass of all training examples\n",
    "num_batches = train_labels.shape[0] // batch_size \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for step in range(num_batches):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            # Generate a minibatch:\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict={training: True, tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            \n",
    "            # Train minibatch\n",
    "            session.run(training_op, feed_dict) \n",
    "        \n",
    "        acc_batch = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "        acc_valid = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: valid_dataset, tf_train_labels: valid_labels})\n",
    "        loss_minibatch = loss.eval(session = session, feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "        print(\"Minibatch loss after epoch %d: %f\" % (epoch+1, loss_minibatch))\n",
    "        print(\"Minibatch accuracy: \", acc_batch, \" | Validation accuracy: \", acc_valid)\n",
    "    \n",
    "    acc_test = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: test_dataset, tf_train_labels: test_labels})\n",
    "    print(\"Test accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's not really an improvement to the simplear case earlier on.\n",
    "\n",
    "Now let's try some other networks then...\n",
    "\n",
    "Here's a deeper network (4 hidden layers with 1000, 400, 100, 80 neurons). And I've added \"early stopping\" to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN + dropout + learning rate decay + early stopping\n",
    "# ELUs (instead of reLUs)\n",
    "\n",
    "# Network architecture:\n",
    "num_neurons_layers = np.array([1000, 400, 100, 80])\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.int64, shape=(None))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    tf_dropout_rate = tf.placeholder(tf.float32, shape=(None), name='tf_dropout_rate')\n",
    "    #tf_initial_learning_rate = tf.placeholder(tf.float32, shape=(None), name='tf_initial_learning_rate')\n",
    "    tf_decay_rate = tf.placeholder(tf.float32, shape=(None), name='tf_decay_rate')    \n",
    "    tf_learning_rate = tf.placeholder(tf.float32, shape=(None), name='tf_decay_rate')  \n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training') #only use DROPOUT during training!\n",
    "    \n",
    "    # Define DNN model\n",
    "    def DNN_model(inputs):\n",
    "        \n",
    "        # go through dense layers:\n",
    "        for layer in range(num_neurons_layers.shape[0]):\n",
    "            num_neurons = num_neurons_layers[layer]\n",
    "            if tf_dropout_rate is not None:\n",
    "                inputs = tf.layers.dropout(inputs, tf_dropout_rate, training=training) \n",
    "            inputs = tf.layers.dense(inputs, num_neurons, activation=tf.nn.elu, name=\"hidden%d\" % (layer + 1))\n",
    "        \n",
    "        # Output\n",
    "        if tf_dropout_rate is not None:\n",
    "            inputs = tf.layers.dropout(inputs, tf_dropout_rate, training=training) \n",
    "        return tf.layers.dense(inputs, num_labels, name=\"outputs\") #10 output layers (logits, before softmax)\n",
    "        \n",
    "    # Training data\n",
    "    logits = DNN_model(tf_train_dataset)\n",
    "    \n",
    "    \n",
    "    # Loss \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    #learning_rate = tf.train.exponential_decay(tf_initial_learning_rate, global_step, decay_steps, tf_decay_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=tf_learning_rate) #optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)#.minimize(loss, global_step=global_step)\n",
    "    training_op = optimizer.minimize(loss) #, global_step=global_step)#.minimize(loss)    \n",
    "    \n",
    "    # Evaluate\n",
    "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(tf_train_labels, 1))\n",
    "    accuracy_measure = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "    \n",
    "    # Save model in early stopping\n",
    "    model_saver = tf.train.Saver()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0001  | Dropout rate:  0.1  | batch_size:  150\n",
      "1\tValidation loss: 0.493366\tBest loss: 0.493366\tAccuracy: 85.15%\n",
      "2\tValidation loss: 0.445149\tBest loss: 0.445149\tAccuracy: 86.32%\n",
      "3\tValidation loss: 0.415768\tBest loss: 0.415768\tAccuracy: 87.41%\n",
      "4\tValidation loss: 0.393848\tBest loss: 0.393848\tAccuracy: 88.13%\n",
      "5\tValidation loss: 0.383416\tBest loss: 0.383416\tAccuracy: 88.38%\n",
      "6\tValidation loss: 0.372152\tBest loss: 0.372152\tAccuracy: 88.75%\n",
      "7\tValidation loss: 0.358697\tBest loss: 0.358697\tAccuracy: 89.07%\n",
      "8\tValidation loss: 0.350609\tBest loss: 0.350609\tAccuracy: 89.38%\n",
      "9\tValidation loss: 0.346526\tBest loss: 0.346526\tAccuracy: 89.58%\n",
      "10\tValidation loss: 0.342017\tBest loss: 0.342017\tAccuracy: 89.65%\n",
      "11\tValidation loss: 0.335934\tBest loss: 0.335934\tAccuracy: 89.96%\n",
      "12\tValidation loss: 0.330833\tBest loss: 0.330833\tAccuracy: 90.07%\n",
      "13\tValidation loss: 0.327956\tBest loss: 0.327956\tAccuracy: 90.35%\n",
      "14\tValidation loss: 0.323960\tBest loss: 0.323960\tAccuracy: 90.20%\n",
      "15\tValidation loss: 0.322006\tBest loss: 0.322006\tAccuracy: 90.35%\n",
      "16\tValidation loss: 0.319885\tBest loss: 0.319885\tAccuracy: 90.52%\n",
      "17\tValidation loss: 0.317864\tBest loss: 0.317864\tAccuracy: 90.46%\n",
      "18\tValidation loss: 0.317060\tBest loss: 0.317060\tAccuracy: 90.54%\n",
      "19\tValidation loss: 0.314399\tBest loss: 0.314399\tAccuracy: 90.67%\n",
      "20\tValidation loss: 0.312460\tBest loss: 0.312460\tAccuracy: 90.68%\n",
      "21\tValidation loss: 0.311904\tBest loss: 0.311904\tAccuracy: 90.63%\n",
      "22\tValidation loss: 0.309161\tBest loss: 0.309161\tAccuracy: 90.98%\n",
      "23\tValidation loss: 0.306402\tBest loss: 0.306402\tAccuracy: 91.04%\n",
      "24\tValidation loss: 0.308583\tBest loss: 0.306402\tAccuracy: 91.15%\n",
      "25\tValidation loss: 0.304554\tBest loss: 0.304554\tAccuracy: 91.10%\n",
      "26\tValidation loss: 0.306451\tBest loss: 0.304554\tAccuracy: 91.09%\n",
      "27\tValidation loss: 0.304452\tBest loss: 0.304452\tAccuracy: 91.14%\n",
      "28\tValidation loss: 0.309275\tBest loss: 0.304452\tAccuracy: 91.11%\n",
      "29\tValidation loss: 0.302203\tBest loss: 0.302203\tAccuracy: 91.27%\n",
      "30\tValidation loss: 0.304140\tBest loss: 0.302203\tAccuracy: 91.18%\n",
      "31\tValidation loss: 0.302238\tBest loss: 0.302203\tAccuracy: 91.26%\n",
      "32\tValidation loss: 0.302865\tBest loss: 0.302203\tAccuracy: 91.18%\n",
      "33\tValidation loss: 0.306167\tBest loss: 0.302203\tAccuracy: 91.35%\n",
      "34\tValidation loss: 0.305093\tBest loss: 0.302203\tAccuracy: 91.32%\n",
      "35\tValidation loss: 0.306711\tBest loss: 0.302203\tAccuracy: 91.39%\n",
      "36\tValidation loss: 0.302180\tBest loss: 0.302180\tAccuracy: 91.40%\n",
      "37\tValidation loss: 0.301114\tBest loss: 0.301114\tAccuracy: 91.45%\n",
      "38\tValidation loss: 0.305736\tBest loss: 0.301114\tAccuracy: 91.35%\n",
      "39\tValidation loss: 0.301362\tBest loss: 0.301114\tAccuracy: 91.45%\n",
      "40\tValidation loss: 0.302125\tBest loss: 0.301114\tAccuracy: 91.25%\n",
      "Test accuracy:  0.9637  | Batch accuracy:  0.946667  | Valid accuracy:  0.9125\n"
     ]
    }
   ],
   "source": [
    "max_checks_without_progress = 10\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "num_epochs = 40 #one epoch = 1x forward pass + backward pass of all training examples\n",
    "#num_batches = train_labels.shape[0] // batch_size \n",
    "\n",
    "# Hyperparameters\n",
    "DROP_rate = [0.1] \n",
    "LEARN_rate = np.array([1e-4], dtype='float')\n",
    "#DECAY_rate = np.array([0.5, 0.25, 0.1], dtype='float32')\n",
    "decay_rate = 1\n",
    "BATCH_size = np.array([150], dtype='int')\n",
    "decay_steps = 5000\n",
    "\n",
    "#Simple (and not very elegant) hyper parameter search:\n",
    "for dropout_rate in DROP_rate:\n",
    "    for learning_rate in LEARN_rate:\n",
    "        for batch_size in BATCH_size:\n",
    "            num_batches = (train_labels.shape[0] // batch_size).astype(int) \n",
    "            \n",
    "            with tf.Session(graph=graph) as session:\n",
    "                tf.global_variables_initializer().run()\n",
    "\n",
    "                print(\"Learning rate: \", learning_rate, \" | Dropout rate: \", dropout_rate, \n",
    "                     \" | batch_size: \", batch_size)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    for step in range(num_batches):\n",
    "                        # Pick an offset within the training data, which has been randomized.\n",
    "                        # Note: we could use better randomization across epochs.\n",
    "                        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                        # Generate a minibatch.\n",
    "                        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "                        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "                        feed_dict={training: True, tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                           tf_learning_rate : learning_rate, tf_decay_rate : decay_rate, \n",
    "                           tf_dropout_rate : dropout_rate}\n",
    "                        \n",
    "                        # Train model:\n",
    "                        session.run(training_op, feed_dict)\n",
    "                   \n",
    "                \n",
    "                    feed_dict={tf_train_dataset: valid_dataset, tf_train_labels: valid_labels, tf_learning_rate : 0, tf_decay_rate : 0, tf_dropout_rate : 0}\n",
    "                    loss_valid, acc_valid = session.run([loss, accuracy_measure], feed_dict)\n",
    "                    if loss_valid < best_loss:\n",
    "                        save_path = model_saver.save(session, \"C:\\\\Programming\\\\Udacity_Deep_Learning\\\\notMNIST_model_01.ckpt\")\n",
    "                        best_loss = loss_valid\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                        if checks_without_progress > max_checks_without_progress:\n",
    "                            print(\"Early stopping!\")\n",
    "                            break\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        (epoch+1), loss_valid, best_loss, acc_valid * 100))\n",
    "\n",
    "                acc_batch = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_learning_rate : 0, tf_decay_rate : 0, tf_dropout_rate : 0})\n",
    "                acc_valid = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: valid_dataset, tf_train_labels: valid_labels, tf_learning_rate : 0, tf_decay_rate : 0, tf_dropout_rate : 0})\n",
    "                acc_test = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: test_dataset, tf_train_labels: test_labels, tf_learning_rate : 0, tf_decay_rate : 0, tf_dropout_rate : 0})\n",
    "                print(\"Test accuracy: \", acc_test, \" | Batch accuracy: \", acc_batch, \" | Valid accuracy: \", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slowly getting real good!\n",
    "\n",
    "For now, let's do one last try by increasing the dropout rate and decreasing the learning rate:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN + dropout + learning rate decay + early stopping\n",
    "# ELUs (instead of reLUs)\n",
    "\n",
    "# Network architecture:\n",
    "num_neurons_layers = np.array([1000, 300, 100, 80])\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.int64, shape=(None))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    tf_dropout_rate = tf.placeholder(tf.float32, shape=(None), name='tf_dropout_rate')\n",
    "    #tf_initial_learning_rate = tf.placeholder(tf.float32, shape=(None), name='tf_initial_learning_rate')\n",
    "    tf_decay_rate = tf.placeholder(tf.float32, shape=(None), name='tf_decay_rate')    \n",
    "    tf_learning_rate = tf.placeholder(tf.float32, shape=(None), name='tf_decay_rate')  \n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training') #only use DROPOUT during training!\n",
    "    \n",
    "    # Define DNN model\n",
    "    def DNN_model(inputs):\n",
    "        \n",
    "        # go through dense layers:\n",
    "        for layer in range(num_neurons_layers.shape[0]):\n",
    "            num_neurons = num_neurons_layers[layer]\n",
    "            if tf_dropout_rate is not None:\n",
    "                inputs = tf.layers.dropout(inputs, tf_dropout_rate, training=training) \n",
    "            inputs = tf.layers.dense(inputs, num_neurons, activation=tf.nn.elu, name=\"hidden%d\" % (layer + 1))\n",
    "        \n",
    "        # Output\n",
    "        if tf_dropout_rate is not None:\n",
    "            inputs = tf.layers.dropout(inputs, tf_dropout_rate, training=training) \n",
    "        return tf.layers.dense(inputs, num_labels, name=\"outputs\") #10 output layers (logits, before softmax)\n",
    "        \n",
    "    # Training data\n",
    "    logits = DNN_model(tf_train_dataset)\n",
    "    \n",
    "    \n",
    "    # Loss \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    #learning_rate = tf.train.exponential_decay(tf_initial_learning_rate, global_step, decay_steps, tf_decay_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=tf_learning_rate) #optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)#.minimize(loss, global_step=global_step)\n",
    "    training_op = optimizer.minimize(loss) #, global_step=global_step)#.minimize(loss)    \n",
    "    \n",
    "    # Evaluate\n",
    "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(tf_train_labels, 1))\n",
    "    accuracy_measure = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "    \n",
    "    # Save model in early stopping\n",
    "    model_saver = tf.train.Saver()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0005  | Dropout rate:  0.3  | batch_size:  150\n",
      "1\tValidation loss: 0.493042\tBest loss: 0.493042\tAccuracy: 84.67%\n",
      "2\tValidation loss: 0.439880\tBest loss: 0.439880\tAccuracy: 86.15%\n",
      "3\tValidation loss: 0.411139\tBest loss: 0.411139\tAccuracy: 87.14%\n",
      "4\tValidation loss: 0.396362\tBest loss: 0.396362\tAccuracy: 87.40%\n",
      "5\tValidation loss: 0.377653\tBest loss: 0.377653\tAccuracy: 88.17%\n",
      "6\tValidation loss: 0.366886\tBest loss: 0.366886\tAccuracy: 88.48%\n",
      "7\tValidation loss: 0.356058\tBest loss: 0.356058\tAccuracy: 88.74%\n",
      "8\tValidation loss: 0.346375\tBest loss: 0.346375\tAccuracy: 89.03%\n",
      "9\tValidation loss: 0.338912\tBest loss: 0.338912\tAccuracy: 89.27%\n",
      "10\tValidation loss: 0.335615\tBest loss: 0.335615\tAccuracy: 89.38%\n",
      "11\tValidation loss: 0.332890\tBest loss: 0.332890\tAccuracy: 89.62%\n",
      "12\tValidation loss: 0.332380\tBest loss: 0.332380\tAccuracy: 89.67%\n",
      "13\tValidation loss: 0.326014\tBest loss: 0.326014\tAccuracy: 89.96%\n",
      "14\tValidation loss: 0.318966\tBest loss: 0.318966\tAccuracy: 90.19%\n",
      "15\tValidation loss: 0.321291\tBest loss: 0.318966\tAccuracy: 90.09%\n",
      "16\tValidation loss: 0.318013\tBest loss: 0.318013\tAccuracy: 90.24%\n",
      "17\tValidation loss: 0.313069\tBest loss: 0.313069\tAccuracy: 90.31%\n",
      "18\tValidation loss: 0.313964\tBest loss: 0.313069\tAccuracy: 90.26%\n",
      "19\tValidation loss: 0.309367\tBest loss: 0.309367\tAccuracy: 90.54%\n",
      "20\tValidation loss: 0.307041\tBest loss: 0.307041\tAccuracy: 90.58%\n",
      "21\tValidation loss: 0.303894\tBest loss: 0.303894\tAccuracy: 90.51%\n",
      "22\tValidation loss: 0.306156\tBest loss: 0.303894\tAccuracy: 90.71%\n",
      "23\tValidation loss: 0.301916\tBest loss: 0.301916\tAccuracy: 90.82%\n",
      "24\tValidation loss: 0.296990\tBest loss: 0.296990\tAccuracy: 90.84%\n",
      "25\tValidation loss: 0.300998\tBest loss: 0.296990\tAccuracy: 90.74%\n",
      "26\tValidation loss: 0.299652\tBest loss: 0.296990\tAccuracy: 90.94%\n",
      "27\tValidation loss: 0.298174\tBest loss: 0.296990\tAccuracy: 90.80%\n",
      "28\tValidation loss: 0.297741\tBest loss: 0.296990\tAccuracy: 90.77%\n",
      "29\tValidation loss: 0.298228\tBest loss: 0.296990\tAccuracy: 90.79%\n",
      "30\tValidation loss: 0.296464\tBest loss: 0.296464\tAccuracy: 90.79%\n",
      "31\tValidation loss: 0.294686\tBest loss: 0.294686\tAccuracy: 91.00%\n",
      "32\tValidation loss: 0.297333\tBest loss: 0.294686\tAccuracy: 90.90%\n",
      "33\tValidation loss: 0.292549\tBest loss: 0.292549\tAccuracy: 91.10%\n",
      "34\tValidation loss: 0.294077\tBest loss: 0.292549\tAccuracy: 90.99%\n",
      "35\tValidation loss: 0.296855\tBest loss: 0.292549\tAccuracy: 90.93%\n",
      "36\tValidation loss: 0.295153\tBest loss: 0.292549\tAccuracy: 91.11%\n",
      "37\tValidation loss: 0.292728\tBest loss: 0.292549\tAccuracy: 91.14%\n",
      "38\tValidation loss: 0.291777\tBest loss: 0.291777\tAccuracy: 90.96%\n",
      "39\tValidation loss: 0.292731\tBest loss: 0.291777\tAccuracy: 91.03%\n",
      "40\tValidation loss: 0.295150\tBest loss: 0.291777\tAccuracy: 90.97%\n",
      "41\tValidation loss: 0.291665\tBest loss: 0.291665\tAccuracy: 91.14%\n",
      "42\tValidation loss: 0.292205\tBest loss: 0.291665\tAccuracy: 91.05%\n",
      "43\tValidation loss: 0.290732\tBest loss: 0.290732\tAccuracy: 91.18%\n",
      "44\tValidation loss: 0.291663\tBest loss: 0.290732\tAccuracy: 91.05%\n",
      "45\tValidation loss: 0.289439\tBest loss: 0.289439\tAccuracy: 91.10%\n",
      "46\tValidation loss: 0.285148\tBest loss: 0.285148\tAccuracy: 91.12%\n",
      "47\tValidation loss: 0.288797\tBest loss: 0.285148\tAccuracy: 91.16%\n",
      "48\tValidation loss: 0.289000\tBest loss: 0.285148\tAccuracy: 91.34%\n",
      "49\tValidation loss: 0.288632\tBest loss: 0.285148\tAccuracy: 91.35%\n",
      "50\tValidation loss: 0.286165\tBest loss: 0.285148\tAccuracy: 91.42%\n",
      "51\tValidation loss: 0.289442\tBest loss: 0.285148\tAccuracy: 91.35%\n",
      "52\tValidation loss: 0.288909\tBest loss: 0.285148\tAccuracy: 91.31%\n",
      "53\tValidation loss: 0.286054\tBest loss: 0.285148\tAccuracy: 91.47%\n",
      "54\tValidation loss: 0.287499\tBest loss: 0.285148\tAccuracy: 91.35%\n",
      "55\tValidation loss: 0.290744\tBest loss: 0.285148\tAccuracy: 91.19%\n",
      "56\tValidation loss: 0.292398\tBest loss: 0.285148\tAccuracy: 91.33%\n",
      "Early stopping!\n",
      "Test accuracy:  0.9656  | Batch accuracy:  0.913333  | Valid accuracy:  0.9118\n"
     ]
    }
   ],
   "source": [
    "max_checks_without_progress = 10\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "num_epochs = 70 #one epoch = 1x forward pass + backward pass of all training examples\n",
    "#num_batches = train_labels.shape[0] // batch_size \n",
    "\n",
    "# Hyperparameters\n",
    "DROP_rate = [0.3] \n",
    "LEARN_rate = np.array([5e-4], dtype='float')\n",
    "#DECAY_rate = np.array([0.5, 0.25, 0.1], dtype='float32')\n",
    "decay_rate = 1\n",
    "BATCH_size = np.array([150], dtype='int')\n",
    "\n",
    "#Simple (and not very elegant) hyper parameter search:\n",
    "for dropout_rate in DROP_rate:\n",
    "    for learning_rate in LEARN_rate:\n",
    "        for batch_size in BATCH_size:\n",
    "            num_batches = (train_labels.shape[0] // batch_size).astype(int) \n",
    "            \n",
    "            with tf.Session(graph=graph) as session:\n",
    "                tf.global_variables_initializer().run()\n",
    "\n",
    "                print(\"Learning rate: \", learning_rate, \" | Dropout rate: \", dropout_rate, \n",
    "                     \" | batch_size: \", batch_size)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    for step in range(num_batches):\n",
    "                        # Pick an offset within the training data, which has been randomized.\n",
    "                        # Note: we could use better randomization across epochs.\n",
    "                        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                        # Generate a minibatch.\n",
    "                        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "                        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "                        feed_dict={training: True, tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                           tf_learning_rate : learning_rate, tf_decay_rate : decay_rate, \n",
    "                           tf_dropout_rate : dropout_rate}\n",
    "                        \n",
    "                        # Train model:\n",
    "                        session.run(training_op, feed_dict)\n",
    "                   \n",
    "                \n",
    "                    feed_dict={tf_train_dataset: valid_dataset, tf_train_labels: valid_labels, tf_learning_rate : 0, tf_decay_rate : 0, tf_dropout_rate : 0}\n",
    "                    loss_valid, acc_valid = session.run([loss, accuracy_measure], feed_dict)\n",
    "                    if loss_valid < best_loss:\n",
    "                        save_path = model_saver.save(session, \"C:\\\\Programming\\\\Udacity_Deep_Learning\\\\notMNIST_model_01.ckpt\")\n",
    "                        best_loss = loss_valid\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                        if checks_without_progress > max_checks_without_progress:\n",
    "                            print(\"Early stopping!\")\n",
    "                            break\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        (epoch+1), loss_valid, best_loss, acc_valid * 100))\n",
    "\n",
    "                acc_batch = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_learning_rate : 0, tf_decay_rate : 0, tf_dropout_rate : 0})\n",
    "                acc_valid = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: valid_dataset, tf_train_labels: valid_labels, tf_learning_rate : 0, tf_decay_rate : 0, tf_dropout_rate : 0})\n",
    "                acc_test = accuracy_measure.eval(session = session, feed_dict={tf_train_dataset: test_dataset, tf_train_labels: test_labels, tf_learning_rate : 0, tf_decay_rate : 0, tf_dropout_rate : 0})\n",
    "                print(\"Test accuracy: \", acc_test, \" | Batch accuracy: \", acc_batch, \" | Valid accuracy: \", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks fairly OK. 96,56 % accuracy for a not very clean dataset (at least not as clean as the classical MNIST).\n",
    "I'll come back to it later and see if I can further improve stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
